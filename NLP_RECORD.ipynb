{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu90UmLsWMGf8FhHTvOP7H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11239A086/NLP_RECORD/blob/main/NLP_RECORD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZATION**"
      ],
      "metadata": {
        "id": "rkPpwzeLnYEH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "295e67bb",
        "outputId": "9e55d718-a112-4880-cccd-ce5a7a371045"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "text = \"I love Natural Language Processing.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMATIZATION**"
      ],
      "metadata": {
        "id": "NGXr2RMEsE2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lm = WordNetLemmatizer()\n",
        "print(lm.lemmatize(\"swimming\", pos='v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "irxahOQh3jkR",
        "outputId": "ae818585-10cf-43c8-e23a-d38f7164cce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swim\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING**"
      ],
      "metadata": {
        "id": "d627rQYlsNgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"playing\", \"played\", \"plays\"]\n",
        "print([ps.stem(w) for w in words])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRv3_o6I6p1_",
        "outputId": "736f2b2f-a26e-4544-b88e-470e598dd6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPELL** **CORRECTION**"
      ],
      "metadata": {
        "id": "ZSxt20B8sDBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "words = [\"apple\", \"banana\", \"orange\", \"grape\", \"mango\"]  # Dictionary\n",
        "word = \"magno\"       # Misspelled word\n",
        "correct = difflib.get_close_matches(word, words, n=1)[0]\n",
        "print(correct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03enyZKaJYiU",
        "outputId": "8df2477e-4aa1-4340-c6c7-4bd4ca49d6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mango\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEDUCTION**"
      ],
      "metadata": {
        "id": "dcq461i7jmKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem(text):\n",
        "    return [ps.stem(w) for w in word_tokenize(text.lower())]\n",
        "\n",
        "def deduce(p, h):\n",
        "    return \"entailment\" if h in p else \"no entailment\"\n",
        "\n",
        "print(\"Stems:\", stem(\"running runner runs easily fairer\"))\n",
        "print(\"Deduction:\", deduce(\"All men are mortal Socrates is a man\", \"Socrates is mortal\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erwmmJwNJtHm",
        "outputId": "0dadd895-917d-4068-d78d-68b17f0f7b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stems: ['run', 'runner', 'run', 'easili', 'fairer']\n",
            "Deduction: no entailment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MORPHOLOGY**"
      ],
      "metadata": {
        "id": "lJ4_5um9sU33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English tokenizer and tagger\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"The children are playing in the park.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Display morphological features for each token\n",
        "for token in doc:\n",
        "    print(f\"Word: {token.text}\")\n",
        "    print(f\"  Lemma: {token.lemma_}\")\n",
        "    print(f\"  POS: {token.pos_}\")\n",
        "    print(f\"  Morph: {token.morph}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NsdhJCaf82fd",
        "outputId": "c3a2f7ca-7819-4602-f909-73bc58aa0db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: The\n",
            "  Lemma: the\n",
            "  POS: DET\n",
            "  Morph: Definite=Def|PronType=Art\n",
            "------------------------------\n",
            "Word: children\n",
            "  Lemma: child\n",
            "  POS: NOUN\n",
            "  Morph: Number=Plur\n",
            "------------------------------\n",
            "Word: are\n",
            "  Lemma: be\n",
            "  POS: AUX\n",
            "  Morph: Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "------------------------------\n",
            "Word: playing\n",
            "  Lemma: play\n",
            "  POS: VERB\n",
            "  Morph: Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "------------------------------\n",
            "Word: in\n",
            "  Lemma: in\n",
            "  POS: ADP\n",
            "  Morph: \n",
            "------------------------------\n",
            "Word: the\n",
            "  Lemma: the\n",
            "  POS: DET\n",
            "  Morph: Definite=Def|PronType=Art\n",
            "------------------------------\n",
            "Word: park\n",
            "  Lemma: park\n",
            "  POS: NOUN\n",
            "  Morph: Number=Sing\n",
            "------------------------------\n",
            "Word: .\n",
            "  Lemma: .\n",
            "  POS: PUNCT\n",
            "  Morph: PunctType=Peri\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-GRAMS"
      ],
      "metadata": {
        "id": "fiI7DxNbiT83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gwk3fqDZxCt",
        "outputId": "0f246723-336f-40dd-ca2b-d1b89a1a1147",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i', 'love', 'natural'): 0.5000\n",
            "('love', 'natural', 'language'): 1.0000\n",
            "('natural', 'language', 'processing'): 1.0000\n",
            "('language', 'processing', 'and'): 1.0000\n",
            "('processing', 'and', 'i'): 1.0000\n",
            "('and', 'i', 'love'): 1.0000\n",
            "('i', 'love', 'machine'): 0.5000\n",
            "('love', 'machine', 'learning'): 1.0000\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import ngrams, FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def ngram_probability_nltk(text, n):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Generate n-grams and (n-1)-grams\n",
        "    ngrams_list = list(ngrams(words, n))\n",
        "    n_minus_1_list = list(ngrams(words, n-1)) if n > 1 else None\n",
        "\n",
        "    # Frequency counts\n",
        "    ngram_freq = FreqDist(ngrams_list)\n",
        "    if n > 1:\n",
        "        n_minus_1_freq = FreqDist(n_minus_1_list)\n",
        "\n",
        "    probabilities = {}\n",
        "    for gram in ngram_freq:\n",
        "        if n == 1:\n",
        "            # Unigram probability\n",
        "            probabilities[gram] = ngram_freq[gram] / len(words)\n",
        "        else:\n",
        "            # Bigram/Trigram probability\n",
        "            probabilities[gram] = ngram_freq[gram] / n_minus_1_freq[gram[:-1]]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "# Example usage\n",
        "text = \"I love natural language processing and I love machine learning\"\n",
        "n = 3  # Change to 1 for unigram, 2 for bigram, 3 for trigram\n",
        "\n",
        "prob = ngram_probability_nltk(text, n)\n",
        "for gram, p in prob.items():\n",
        "    print(f\"{gram}: {p:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80c19ad9",
        "outputId": "6b1ff322-60f3-449d-f4a7-aab546aad6aa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SMOOTHING** **N-GRAM** (**LAPLACE** **SMOOTHING**)"
      ],
      "metadata": {
        "id": "b0ossHKpjBLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_smooth(text, n=2):\n",
        "    words = text.split()\n",
        "    N = len(words) - n + 1\n",
        "    ngrams = [tuple(words[i:i+n]) for i in range(N)]\n",
        "    V = len(set(ngrams))\n",
        "    for ng in sorted(set(ngrams)):\n",
        "        count = ngrams.count(ng)\n",
        "        print(ng, \"->\", round((count+1)/(N+V), 4))\n",
        "\n",
        "text = \"I love NLP and I love Python\"\n",
        "ngram_smooth(text, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqQqt-avjAJe",
        "outputId": "469c612e-1643-421f-812c-82cffb0c76ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'love') -> 0.2727\n",
            "('NLP', 'and') -> 0.1818\n",
            "('and', 'I') -> 0.1818\n",
            "('love', 'NLP') -> 0.1818\n",
            "('love', 'Python') -> 0.1818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS** **TAGGING**"
      ],
      "metadata": {
        "id": "LTJS28M8vRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print word and its POS tag\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjO3VvhkJpIy",
        "outputId": "4f905130-7483-48a6-f59f-9646fa338a43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → DET\n",
            "quick → ADJ\n",
            "brown → ADJ\n",
            "fox → NOUN\n",
            "jumps → VERB\n",
            "over → ADP\n",
            "the → DET\n",
            "lazy → ADJ\n",
            "dog → NOUN\n",
            ". → PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BENDING** **POS** **TAGGER**"
      ],
      "metadata": {
        "id": "z9du5A8tvabm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import brill, BrillTaggerTrainer\n",
        "\n",
        "# Training data (small example)\n",
        "train_data = [\n",
        "    [('The','DT'),('dog','NN'),('barks','VBZ')],\n",
        "    [('A','DT'),('cat','NN'),('meows','VBZ')]\n",
        "]\n",
        "\n",
        "# Base tagger (Unigram)\n",
        "base_tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "# Brill trainer with fntbl37 templates\n",
        "trainer = BrillTaggerTrainer(base_tagger, brill.fntbl37())\n",
        "brill_tagger = trainer.train(train_data)\n",
        "\n",
        "# Test\n",
        "sentence = ['The','cat','barks']\n",
        "print(brill_tagger.tag(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYe7Q--IJ-KE",
        "outputId": "22d672cc-9d22-4b16-afda-424164de80ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('cat', 'NN'), ('barks', 'VBZ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hidden** **Markov** **Model**"
      ],
      "metadata": {
        "id": "pKBIROxTvhf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Training data\n",
        "tagged_sentences = treebank.tagged_sents()[:3000]\n",
        "test_sentences = treebank.tagged_sents()[3000:]\n",
        "\n",
        "# Train Hidden Markov Model\n",
        "from nltk.tag import hmm\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_model = trainer.train_supervised(tagged_sentences)\n",
        "\n",
        "# Test the model\n",
        "print(\"Predicted tags:\", hmm_model.tag([\"This\", \"is\", \"a\", \"test\"]))\n",
        "print(\"Accuracy:\", hmm_model.evaluate(test_sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPeLYP7pKE9E",
        "outputId": "ed8f1902-cbdf-42ec-f4dc-5d85ba5d6cd2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('test', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2960517171.py:15: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(\"Accuracy:\", hmm_model.evaluate(test_sentences))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.36844377293330455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8205c75",
        "outputId": "a73a20eb-7934-4a58-e9d1-f5fc2de3e28f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NORMALIZATION**"
      ],
      "metadata": {
        "id": "LebW9hdCdBtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "text = \"running runners ran easily fairer\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "norm = [lem.lemmatize(w) for w in tokens]\n",
        "\n",
        "print(\"Original:\", tokens)\n",
        "print(\"Normalized:\", norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y31Q74lYdFBM",
        "outputId": "ebe1e84e-6797-437a-bfdf-134fe066d7c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['running', 'runners', 'ran', 'easily', 'fairer']\n",
            "Normalized: ['running', 'runner', 'ran', 'easily', 'fairer']\n"
          ]
        }
      ]
    }
  ]
}